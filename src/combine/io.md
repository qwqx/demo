五种

所以说，当一个read操作发生时，它会经历两个阶段：

第一阶段：等待数据准备 (Waiting for the data to be ready)。
 第二阶段：将数据从内核拷贝到进程中 (Copying the data from the kernel to the process)。

## 1.阻塞io

使用系统调用，并一直阻塞直到内核将数据准备好，之后再由内核缓冲区复制到用户态，在等待内核准备的这段时间什么也干不了

## 2.非阻塞io

内核在没有准备好数据的时候会返回错误码，而调用程序不会休眠，而是不断轮询询问内核数据是否准备好

非阻塞式IO的轮询会耗费大量cpu

## 3.io多路复用

类似与非阻塞，只不过轮询不是由用户线程去执行，而是由内核去轮询，内核监听程序监听到数据准备好后，调用内核函数复制数据到用户态，通过回调的方式通知用户线程

由于它可以同时监听很多套接字，所以性能比前两者高

多路复用包括：

- select：线性扫描所有监听的文件描述符，不管他们是不是活跃的。有最大数量限制（32位系统1024，64位系统2048）

- poll：同select，不过数据结构不同，需要分配一个pollfd结构数组，维护在内核中。它没有大小限制，不过需要很多复制操作

- epoll：用于代替poll和select，没有大小限制。使用一个文件描述符管理多个文件描述符，使用红黑树存储。同时用事件驱动代替了轮询。epoll_ctl中注册的文件描述符在事件触发的时候会通过回调机制激活该文件描述符。epoll_wait便会收到通知。最后，epoll还采用了mmap虚拟内存映射技术减少用户态和内核态数据传输的开销

  使用 select 和 poll 时，我们发现了以下三个问题：

  1. 如何突破文件描述符数量的限制（poll 已经解决）
  2. 如何避免用户态和内核态对文件描述符集合的拷贝
  3. 如何避免线性遍历文件描述符集合

  针对第一点，epoll 采用了红黑树的结构存储文件描述符，这样来在性能上与 poll 的链表相比有了很大的提升。对于第二点，epoll 只开辟了一个空间，即只用内核来存放文件描述符，用户态也是操作这一份空间。

  对于第三点，epoll 新增了一个 socket 就绪链表和回调函数。当连接(socket) 有数据时，会自动触发回调函数，将就绪的 socket 放到链表中，同时告诉用户态过来处理。这样的话内核就不用一直去轮询符号表了，实现了 O(1) 的复杂度。

## 4.信号驱动式io

使用信号，内核在数据准备就绪时通过信号来进行通知

首先开启信号驱动io套接字，并使用sigaction系统调用来安装信号处理程序，内核直接返回，不会阻塞用户态

数据准备好时，内核会发送SIGIO信号，收到信号后开始进行io操作

## 5.异步io

异步IO依赖信号处理程序来进行通知

不过异步IO与前面IO模型不同的是：前面的都是数据准备阶段的阻塞与非阻塞，异步IO模型通知的是IO操作已经完成，而不是数据准备完成

异步IO才是真正的非阻塞，主进程只负责做自己的事情，等IO操作完成(数据成功从内核缓存区复制到应用程序缓冲区)时通过回调函数对数据进行处理



## 6.对比

- 前面四种IO模型的主要区别在第一阶段，他们第二阶段是一样的：数据从内核缓冲区复制到调用者缓冲区期间都被阻塞住！
- 前面四种IO都是同步IO：IO操作导致请求进程阻塞，直到IO操作完成
- 异步IO：IO操作不导致请求进程阻塞

